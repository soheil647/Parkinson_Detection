{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_data(path = 'pd_speech_features.csv'):\n",
    "    df = pd.read_csv(path).drop('id', axis=1)\n",
    "    display(df)\n",
    "    print(df.info(), Counter(df['class']))\n",
    "    \n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_data(X):\n",
    "    # scale data\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanlde Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Oversampling\n",
    "def oversample(in_data, target, sampling_strategy='minority'):\n",
    "    oversample = RandomOverSampler(sampling_strategy= sampling_strategy)\n",
    "    in_data_over, target_over = oversample.fit_resample(in_data, target)\n",
    "    return in_data_over, target_over\n",
    "\n",
    "# Undersampling\n",
    "def undersample(in_data, target, sampling_strategy= 'majority'):\n",
    "    undersample = RandomUnderSampler(sampling_strategy= sampling_strategy)\n",
    "    in_data_under, target_under = undersample.fit_resample(in_data, target)\n",
    "    return in_data_under, target_under\n",
    "\n",
    "# SMOTE\n",
    "def smote(in_data, target, sampling_strategy= 'minority'):\n",
    "    smote = SMOTE(sampling_strategy= sampling_strategy)\n",
    "    in_data_smote, target_smote = smote.fit_resample(in_data, target)\n",
    "    return in_data_smote, target_smote\n",
    "\n",
    "# Function to Handle The Method\n",
    "def handle_imbalanced(in_data, target, method, sampling_strategy= 'minority'):\n",
    "    if method == 'oversample':\n",
    "        return oversample(in_data, target)\n",
    "    elif method == 'undersample':\n",
    "        return undersample(in_data, target)\n",
    "    elif method == 'smote':\n",
    "        return smote(in_data, target)\n",
    "    else:\n",
    "        return in_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 564, 0: 564})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_data, balance_target = handle_imbalanced(X, y, 'smote')\n",
    "Counter(balance_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality Reduction Techniqes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Function\n",
    "def pca_func(in_data, n_components, whitening = False, svd_solver = 'full', plot = True):\n",
    "    #draw a plot\n",
    "    if plot:\n",
    "        pca_base = PCA().fit(in_data)\n",
    "        plt.plot(pca_base.explained_variance_ratio_)\n",
    "        plt.xlabel('n_components')\n",
    "        plt.ylabel('Variance')\n",
    "        plt.show()\n",
    "    #main PCA\n",
    "    out_data = PCA(n_components = n_components, svd_solver = svd_solver, whiten = whitening).fit_transform(in_data)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Function\n",
    "def lda_func(in_data, in_target, n_components, solver = 'svd'):\n",
    "    out_data = LinearDiscriminantAnalysis(n_components = n_components, solver = solver).fit_transform(in_data, in_target)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA Function\n",
    "def ica_func(in_data, n_components):\n",
    "    out_data = FastICA(n_components = n_components).fit_transform(in_data)\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "64/64 [==============================] - 0s 7ms/step - loss: 0.1148 - accuracy: 0.0177 - val_loss: 0.0588 - val_accuracy: 0.0177\n",
      "Epoch 2/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.0217 - val_loss: 0.0515 - val_accuracy: 0.0177\n",
      "Epoch 3/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 0.0256 - val_loss: 0.0437 - val_accuracy: 0.0177\n",
      "Epoch 4/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0505 - accuracy: 0.0276 - val_loss: 0.0403 - val_accuracy: 0.0177\n",
      "Epoch 5/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.0276 - val_loss: 0.0401 - val_accuracy: 0.0088\n",
      "Epoch 6/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0454 - accuracy: 0.0315 - val_loss: 0.0366 - val_accuracy: 0.0177\n",
      "Epoch 7/300\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0433 - accuracy: 0.0246 - val_loss: 0.0358 - val_accuracy: 0.0088\n",
      "Epoch 8/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.0246 - val_loss: 0.0345 - val_accuracy: 0.0265\n",
      "Epoch 9/300\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.0227 - val_loss: 0.0341 - val_accuracy: 0.0177\n",
      "Epoch 10/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.0227 - val_loss: 0.0332 - val_accuracy: 0.0265\n",
      "Epoch 11/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0399 - accuracy: 0.0246 - val_loss: 0.0325 - val_accuracy: 0.0265\n",
      "Epoch 12/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.0286 - val_loss: 0.0322 - val_accuracy: 0.0265\n",
      "Epoch 13/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.0296 - val_loss: 0.0318 - val_accuracy: 0.0265\n",
      "Epoch 14/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.0325 - val_loss: 0.0305 - val_accuracy: 0.0354\n",
      "Epoch 15/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.0374 - val_loss: 0.0304 - val_accuracy: 0.0265\n",
      "Epoch 16/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.0394 - val_loss: 0.0294 - val_accuracy: 0.0177\n",
      "Epoch 17/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.0355 - val_loss: 0.0297 - val_accuracy: 0.0177\n",
      "Epoch 18/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.0335 - val_loss: 0.0289 - val_accuracy: 0.0177\n",
      "Epoch 19/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.0335 - val_loss: 0.0280 - val_accuracy: 0.0177\n",
      "Epoch 20/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.0315 - val_loss: 0.0280 - val_accuracy: 0.0177\n",
      "Epoch 21/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0343 - accuracy: 0.0325 - val_loss: 0.0280 - val_accuracy: 0.0177\n",
      "Epoch 22/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.0305 - val_loss: 0.0272 - val_accuracy: 0.0177\n",
      "Epoch 23/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.0325 - val_loss: 0.0278 - val_accuracy: 0.0177\n",
      "Epoch 24/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.0325 - val_loss: 0.0274 - val_accuracy: 0.0177\n",
      "Epoch 25/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.0355 - val_loss: 0.0264 - val_accuracy: 0.0177\n",
      "Epoch 26/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.0365 - val_loss: 0.0264 - val_accuracy: 0.0177\n",
      "Epoch 27/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.0345 - val_loss: 0.0258 - val_accuracy: 0.0177\n",
      "Epoch 28/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0345 - val_loss: 0.0260 - val_accuracy: 0.0177\n",
      "Epoch 29/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0345 - val_loss: 0.0256 - val_accuracy: 0.0177\n",
      "Epoch 30/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0355 - val_loss: 0.0257 - val_accuracy: 0.0177\n",
      "Epoch 31/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.0345 - val_loss: 0.0251 - val_accuracy: 0.0177\n",
      "Epoch 32/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.0365 - val_loss: 0.0268 - val_accuracy: 0.0177\n",
      "Epoch 33/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0325 - val_loss: 0.0255 - val_accuracy: 0.0177\n",
      "Epoch 34/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0335 - val_loss: 0.0254 - val_accuracy: 0.0177\n",
      "Epoch 35/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.0355 - val_loss: 0.0249 - val_accuracy: 0.0177\n",
      "Epoch 36/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.0355 - val_loss: 0.0249 - val_accuracy: 0.0177\n",
      "Epoch 37/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0355 - val_loss: 0.0248 - val_accuracy: 0.0177\n",
      "Epoch 38/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.0355 - val_loss: 0.0243 - val_accuracy: 0.0177\n",
      "Epoch 39/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0365 - val_loss: 0.0259 - val_accuracy: 0.0177\n",
      "Epoch 40/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0345 - val_loss: 0.0249 - val_accuracy: 0.0177\n",
      "Epoch 41/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0365 - val_loss: 0.0248 - val_accuracy: 0.0177\n",
      "Epoch 42/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0355 - val_loss: 0.0242 - val_accuracy: 0.0177\n",
      "Epoch 43/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.0374 - val_loss: 0.0238 - val_accuracy: 0.0177\n",
      "Epoch 44/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.0355 - val_loss: 0.0249 - val_accuracy: 0.0177\n",
      "Epoch 45/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.0374 - val_loss: 0.0237 - val_accuracy: 0.0265\n",
      "Epoch 46/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.0394 - val_loss: 0.0240 - val_accuracy: 0.0177\n",
      "Epoch 47/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.0365 - val_loss: 0.0237 - val_accuracy: 0.0177\n",
      "Epoch 48/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.0365 - val_loss: 0.0243 - val_accuracy: 0.0177\n",
      "Epoch 49/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.0374 - val_loss: 0.0238 - val_accuracy: 0.0177\n",
      "Epoch 50/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.0384 - val_loss: 0.0230 - val_accuracy: 0.0177\n",
      "Epoch 51/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.0374 - val_loss: 0.0234 - val_accuracy: 0.0265\n",
      "Epoch 52/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0414 - val_loss: 0.0235 - val_accuracy: 0.0177\n",
      "Epoch 53/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0394 - val_loss: 0.0235 - val_accuracy: 0.0265\n",
      "Epoch 54/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0394 - val_loss: 0.0236 - val_accuracy: 0.0177\n",
      "Epoch 55/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0394 - val_loss: 0.0234 - val_accuracy: 0.0265\n",
      "Epoch 56/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0384 - val_loss: 0.0233 - val_accuracy: 0.0177\n",
      "Epoch 57/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0394 - val_loss: 0.0230 - val_accuracy: 0.0265\n",
      "Epoch 58/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0404 - val_loss: 0.0233 - val_accuracy: 0.0265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0384 - val_loss: 0.0235 - val_accuracy: 0.0265\n",
      "Epoch 60/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0404 - val_loss: 0.0231 - val_accuracy: 0.0265\n",
      "Epoch 61/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0404 - val_loss: 0.0257 - val_accuracy: 0.0265\n",
      "Epoch 62/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0433 - val_loss: 0.0229 - val_accuracy: 0.0265\n",
      "Epoch 63/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0433 - val_loss: 0.0236 - val_accuracy: 0.0177\n",
      "Epoch 64/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0424 - val_loss: 0.0242 - val_accuracy: 0.0177\n",
      "Epoch 65/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0404 - val_loss: 0.0231 - val_accuracy: 0.0265\n",
      "Epoch 66/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0424 - val_loss: 0.0238 - val_accuracy: 0.0177\n",
      "Epoch 67/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0414 - val_loss: 0.0234 - val_accuracy: 0.0265\n",
      "Epoch 68/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0414 - val_loss: 0.0233 - val_accuracy: 0.0354\n",
      "Epoch 69/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0414 - val_loss: 0.0234 - val_accuracy: 0.0265\n",
      "Epoch 70/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0414 - val_loss: 0.0226 - val_accuracy: 0.0177\n",
      "Epoch 71/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0433 - val_loss: 0.0224 - val_accuracy: 0.0265\n",
      "Epoch 72/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0433 - val_loss: 0.0247 - val_accuracy: 0.0265\n",
      "Epoch 73/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.0433 - val_loss: 0.0241 - val_accuracy: 0.0265\n",
      "Epoch 74/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.0424 - val_loss: 0.0240 - val_accuracy: 0.0265\n",
      "Epoch 75/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.0404 - val_loss: 0.0230 - val_accuracy: 0.0265\n",
      "Epoch 76/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0287 - accuracy: 0.0443 - val_loss: 0.0232 - val_accuracy: 0.0265\n",
      "Epoch 77/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0424 - val_loss: 0.0226 - val_accuracy: 0.0265\n",
      "Epoch 78/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.0443 - val_loss: 0.0237 - val_accuracy: 0.0265\n",
      "Epoch 79/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.0463 - val_loss: 0.0222 - val_accuracy: 0.0265\n",
      "Epoch 80/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.0404 - val_loss: 0.0227 - val_accuracy: 0.0265\n",
      "Epoch 81/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.0443 - val_loss: 0.0225 - val_accuracy: 0.0265\n",
      "Epoch 82/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.0453 - val_loss: 0.0226 - val_accuracy: 0.0265\n",
      "Epoch 83/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0443 - val_loss: 0.0226 - val_accuracy: 0.0265\n",
      "Epoch 84/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.0453 - val_loss: 0.0224 - val_accuracy: 0.0177\n",
      "Epoch 85/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.0443 - val_loss: 0.0223 - val_accuracy: 0.0265\n",
      "Epoch 86/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.0433 - val_loss: 0.0227 - val_accuracy: 0.0177\n",
      "Epoch 87/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.0443 - val_loss: 0.0220 - val_accuracy: 0.0354\n",
      "Epoch 88/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0424 - val_loss: 0.0221 - val_accuracy: 0.0354\n",
      "Epoch 89/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0433 - val_loss: 0.0223 - val_accuracy: 0.0265\n",
      "Epoch 90/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.0433 - val_loss: 0.0226 - val_accuracy: 0.0265\n",
      "Epoch 91/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.0433 - val_loss: 0.0221 - val_accuracy: 0.0354\n",
      "Epoch 92/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0433 - val_loss: 0.0221 - val_accuracy: 0.0354\n",
      "Epoch 93/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0443 - val_loss: 0.0221 - val_accuracy: 0.0354\n",
      "Epoch 94/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0424 - val_loss: 0.0220 - val_accuracy: 0.0265\n",
      "Epoch 95/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0424 - val_loss: 0.0222 - val_accuracy: 0.0354\n",
      "Epoch 96/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0433 - val_loss: 0.0226 - val_accuracy: 0.0354\n",
      "Epoch 97/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.0424 - val_loss: 0.0222 - val_accuracy: 0.0265\n",
      "Epoch 98/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0433 - val_loss: 0.0223 - val_accuracy: 0.0265\n",
      "Epoch 99/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.0414 - val_loss: 0.0219 - val_accuracy: 0.0265\n",
      "Epoch 100/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.0433 - val_loss: 0.0216 - val_accuracy: 0.0354\n",
      "Epoch 101/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.0424 - val_loss: 0.0218 - val_accuracy: 0.0354\n",
      "Epoch 102/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.0453 - val_loss: 0.0219 - val_accuracy: 0.0354\n",
      "Epoch 103/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.0424 - val_loss: 0.0213 - val_accuracy: 0.0265\n",
      "Epoch 104/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.0414 - val_loss: 0.0231 - val_accuracy: 0.0265\n",
      "Epoch 105/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0414 - val_loss: 0.0220 - val_accuracy: 0.0354\n",
      "Epoch 106/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.0414 - val_loss: 0.0217 - val_accuracy: 0.0265\n",
      "Epoch 107/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.0443 - val_loss: 0.0218 - val_accuracy: 0.0354\n",
      "Epoch 108/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0414 - val_loss: 0.0223 - val_accuracy: 0.0354\n",
      "Epoch 109/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 0.0414 - val_loss: 0.0215 - val_accuracy: 0.0354\n",
      "Epoch 110/300\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.0424 - val_loss: 0.0214 - val_accuracy: 0.0265\n",
      "Epoch 111/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 0.0404 - val_loss: 0.0215 - val_accuracy: 0.0354\n",
      "Epoch 112/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.0404 - val_loss: 0.0219 - val_accuracy: 0.0265\n",
      "Epoch 113/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0404 - val_loss: 0.0218 - val_accuracy: 0.0177\n",
      "Epoch 114/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0414 - val_loss: 0.0219 - val_accuracy: 0.0354\n",
      "Epoch 115/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0394 - val_loss: 0.0221 - val_accuracy: 0.0265\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0433 - val_loss: 0.0218 - val_accuracy: 0.0354\n",
      "Epoch 117/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.0424 - val_loss: 0.0215 - val_accuracy: 0.0354\n",
      "Epoch 118/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0433 - val_loss: 0.0219 - val_accuracy: 0.0354\n",
      "Epoch 119/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0414 - val_loss: 0.0223 - val_accuracy: 0.0354\n",
      "Epoch 120/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.0424 - val_loss: 0.0213 - val_accuracy: 0.0354\n",
      "Epoch 121/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.0404 - val_loss: 0.0214 - val_accuracy: 0.0354\n",
      "Epoch 122/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.0404 - val_loss: 0.0218 - val_accuracy: 0.0354\n",
      "Epoch 123/300\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 0.0424 - val_loss: 0.0213 - val_accuracy: 0.0354\n",
      "Epoch 124/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.0433 - val_loss: 0.0211 - val_accuracy: 0.0354\n",
      "Epoch 125/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0404 - val_loss: 0.0223 - val_accuracy: 0.0265\n",
      "Epoch 126/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0404 - val_loss: 0.0215 - val_accuracy: 0.0354\n",
      "Epoch 127/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0404 - val_loss: 0.0210 - val_accuracy: 0.0354\n",
      "Epoch 128/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0414 - val_loss: 0.0214 - val_accuracy: 0.0354\n",
      "Epoch 129/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0414 - val_loss: 0.0218 - val_accuracy: 0.0354\n",
      "Epoch 130/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.0404 - val_loss: 0.0212 - val_accuracy: 0.0354\n",
      "Epoch 131/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.0424 - val_loss: 0.0207 - val_accuracy: 0.0354\n",
      "Epoch 132/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0433 - val_loss: 0.0217 - val_accuracy: 0.0265\n",
      "Epoch 133/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0453 - val_loss: 0.0215 - val_accuracy: 0.0177\n",
      "Epoch 134/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0433 - val_loss: 0.0211 - val_accuracy: 0.0177\n",
      "Epoch 135/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0433 - val_loss: 0.0212 - val_accuracy: 0.0354\n",
      "Epoch 136/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0414 - val_loss: 0.0214 - val_accuracy: 0.0265\n",
      "Epoch 137/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0374 - val_loss: 0.0213 - val_accuracy: 0.0265\n",
      "Epoch 138/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0424 - val_loss: 0.0213 - val_accuracy: 0.0265\n",
      "Epoch 139/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0414 - val_loss: 0.0209 - val_accuracy: 0.0265\n",
      "Epoch 140/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0384 - val_loss: 0.0208 - val_accuracy: 0.0265\n",
      "Epoch 141/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0443 - val_loss: 0.0212 - val_accuracy: 0.0354\n",
      "Epoch 142/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0424 - val_loss: 0.0214 - val_accuracy: 0.0265\n",
      "Epoch 143/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0394 - val_loss: 0.0207 - val_accuracy: 0.0265\n",
      "Epoch 144/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0424 - val_loss: 0.0212 - val_accuracy: 0.0265\n",
      "Epoch 145/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0424 - val_loss: 0.0213 - val_accuracy: 0.0354\n",
      "Epoch 146/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.0414 - val_loss: 0.0208 - val_accuracy: 0.0265\n",
      "Epoch 147/300\n",
      "64/64 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.0414 - val_loss: 0.0208 - val_accuracy: 0.0265\n",
      "Epoch 148/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0433 - val_loss: 0.0210 - val_accuracy: 0.0265\n",
      "Epoch 149/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0433 - val_loss: 0.0210 - val_accuracy: 0.0354\n",
      "Epoch 150/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0453 - val_loss: 0.0215 - val_accuracy: 0.0265\n",
      "Epoch 151/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0433 - val_loss: 0.0214 - val_accuracy: 0.0177\n",
      "Epoch 152/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0453 - val_loss: 0.0213 - val_accuracy: 0.0177\n",
      "Epoch 153/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0463 - val_loss: 0.0204 - val_accuracy: 0.0265\n",
      "Epoch 154/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0453 - val_loss: 0.0206 - val_accuracy: 0.0177\n",
      "Epoch 155/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0424 - val_loss: 0.0217 - val_accuracy: 0.0265\n",
      "Epoch 156/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.0483 - val_loss: 0.0214 - val_accuracy: 0.0265\n",
      "Epoch 157/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0443 - val_loss: 0.0208 - val_accuracy: 0.0177\n",
      "Epoch 158/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.0414 - val_loss: 0.0206 - val_accuracy: 0.0177\n",
      "Epoch 159/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0453 - val_loss: 0.0206 - val_accuracy: 0.0177\n",
      "Epoch 160/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0473 - val_loss: 0.0214 - val_accuracy: 0.0265\n",
      "Epoch 161/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0443 - val_loss: 0.0208 - val_accuracy: 0.0354\n",
      "Epoch 162/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0443 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 163/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0463 - val_loss: 0.0203 - val_accuracy: 0.0177\n",
      "Epoch 164/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0483 - val_loss: 0.0207 - val_accuracy: 0.0177\n",
      "Epoch 165/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0463 - val_loss: 0.0205 - val_accuracy: 0.0177\n",
      "Epoch 166/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0473 - val_loss: 0.0207 - val_accuracy: 0.0177\n",
      "Epoch 167/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0453 - val_loss: 0.0206 - val_accuracy: 0.0354\n",
      "Epoch 168/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0512 - val_loss: 0.0206 - val_accuracy: 0.0354\n",
      "Epoch 169/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0453 - val_loss: 0.0201 - val_accuracy: 0.0177\n",
      "Epoch 170/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0483 - val_loss: 0.0205 - val_accuracy: 0.0177\n",
      "Epoch 171/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0502 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 172/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0463 - val_loss: 0.0206 - val_accuracy: 0.0177\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0443 - val_loss: 0.0207 - val_accuracy: 0.0177\n",
      "Epoch 174/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0473 - val_loss: 0.0207 - val_accuracy: 0.0265\n",
      "Epoch 175/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0502 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 176/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.0463 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 177/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0483 - val_loss: 0.0199 - val_accuracy: 0.0265\n",
      "Epoch 178/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0532 - val_loss: 0.0202 - val_accuracy: 0.0265\n",
      "Epoch 179/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.0502 - val_loss: 0.0203 - val_accuracy: 0.0177\n",
      "Epoch 180/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0483 - val_loss: 0.0202 - val_accuracy: 0.0265\n",
      "Epoch 181/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0512 - val_loss: 0.0198 - val_accuracy: 0.0177\n",
      "Epoch 182/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0502 - val_loss: 0.0200 - val_accuracy: 0.0177\n",
      "Epoch 183/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0502 - val_loss: 0.0205 - val_accuracy: 0.0265\n",
      "Epoch 184/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0552 - val_loss: 0.0200 - val_accuracy: 0.0265\n",
      "Epoch 185/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0552 - val_loss: 0.0200 - val_accuracy: 0.0177\n",
      "Epoch 186/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0502 - val_loss: 0.0206 - val_accuracy: 0.0177\n",
      "Epoch 187/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0502 - val_loss: 0.0208 - val_accuracy: 0.0265\n",
      "Epoch 188/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0473 - val_loss: 0.0206 - val_accuracy: 0.0265\n",
      "Epoch 189/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0522 - val_loss: 0.0198 - val_accuracy: 0.0177\n",
      "Epoch 190/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0502 - val_loss: 0.0202 - val_accuracy: 0.0265\n",
      "Epoch 191/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0512 - val_loss: 0.0198 - val_accuracy: 0.0265\n",
      "Epoch 192/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0522 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 193/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0552 - val_loss: 0.0204 - val_accuracy: 0.0265\n",
      "Epoch 194/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0502 - val_loss: 0.0210 - val_accuracy: 0.0265\n",
      "Epoch 195/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0522 - val_loss: 0.0197 - val_accuracy: 0.0177\n",
      "Epoch 196/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0522 - val_loss: 0.0203 - val_accuracy: 0.0265\n",
      "Epoch 197/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0493 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 198/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0522 - val_loss: 0.0195 - val_accuracy: 0.0265\n",
      "Epoch 199/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0512 - val_loss: 0.0196 - val_accuracy: 0.0442\n",
      "Epoch 200/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0522 - val_loss: 0.0204 - val_accuracy: 0.0265\n",
      "Epoch 201/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0522 - val_loss: 0.0199 - val_accuracy: 0.0265\n",
      "Epoch 202/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0542 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 203/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0552 - val_loss: 0.0197 - val_accuracy: 0.0442\n",
      "Epoch 204/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0522 - val_loss: 0.0204 - val_accuracy: 0.0177\n",
      "Epoch 205/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0552 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 206/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0562 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 207/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0522 - val_loss: 0.0199 - val_accuracy: 0.0354\n",
      "Epoch 208/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0522 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 209/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0552 - val_loss: 0.0201 - val_accuracy: 0.0354\n",
      "Epoch 210/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0532 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 211/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0562 - val_loss: 0.0200 - val_accuracy: 0.0265\n",
      "Epoch 212/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0542 - val_loss: 0.0200 - val_accuracy: 0.0177\n",
      "Epoch 213/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0532 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 214/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0552 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 215/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0532 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 216/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0562 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 217/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0542 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 218/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0542 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 219/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0571 - val_loss: 0.0195 - val_accuracy: 0.0442\n",
      "Epoch 220/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0562 - val_loss: 0.0193 - val_accuracy: 0.0265\n",
      "Epoch 221/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0552 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 222/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0581 - val_loss: 0.0192 - val_accuracy: 0.0265\n",
      "Epoch 223/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0552 - val_loss: 0.0197 - val_accuracy: 0.0354\n",
      "Epoch 224/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0571 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 225/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0571 - val_loss: 0.0198 - val_accuracy: 0.0354\n",
      "Epoch 226/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0571 - val_loss: 0.0194 - val_accuracy: 0.0265\n",
      "Epoch 227/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0581 - val_loss: 0.0194 - val_accuracy: 0.0442\n",
      "Epoch 228/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0562 - val_loss: 0.0207 - val_accuracy: 0.0265\n",
      "Epoch 229/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0532 - val_loss: 0.0195 - val_accuracy: 0.0442\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0552 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 231/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0581 - val_loss: 0.0201 - val_accuracy: 0.0265\n",
      "Epoch 232/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0542 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 233/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0581 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 234/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0542 - val_loss: 0.0199 - val_accuracy: 0.0354\n",
      "Epoch 235/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0562 - val_loss: 0.0191 - val_accuracy: 0.0442\n",
      "Epoch 236/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0571 - val_loss: 0.0196 - val_accuracy: 0.0442\n",
      "Epoch 237/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0591 - val_loss: 0.0198 - val_accuracy: 0.0354\n",
      "Epoch 238/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0581 - val_loss: 0.0193 - val_accuracy: 0.0354\n",
      "Epoch 239/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.0571 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 240/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.0581 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 241/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0542 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 242/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0552 - val_loss: 0.0202 - val_accuracy: 0.0354\n",
      "Epoch 243/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0552 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 244/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0552 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 245/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0571 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 246/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0601 - val_loss: 0.0197 - val_accuracy: 0.0354\n",
      "Epoch 247/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0591 - val_loss: 0.0191 - val_accuracy: 0.0265\n",
      "Epoch 248/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0591 - val_loss: 0.0188 - val_accuracy: 0.0265\n",
      "Epoch 249/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0591 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 250/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0562 - val_loss: 0.0190 - val_accuracy: 0.0265\n",
      "Epoch 251/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0552 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 252/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0591 - val_loss: 0.0197 - val_accuracy: 0.0354\n",
      "Epoch 253/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0552 - val_loss: 0.0188 - val_accuracy: 0.0354\n",
      "Epoch 254/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0591 - val_loss: 0.0188 - val_accuracy: 0.0354\n",
      "Epoch 255/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0552 - val_loss: 0.0196 - val_accuracy: 0.0354\n",
      "Epoch 256/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0562 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 257/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0601 - val_loss: 0.0191 - val_accuracy: 0.0442\n",
      "Epoch 258/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0571 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 259/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0591 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 260/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0552 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 261/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0581 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 262/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0571 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 263/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0571 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 264/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0621 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 265/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0591 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 266/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0571 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 267/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0562 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 268/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0581 - val_loss: 0.0188 - val_accuracy: 0.0354\n",
      "Epoch 269/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0571 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 270/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0591 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 271/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0601 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 272/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0601 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 273/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0601 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 274/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0631 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 275/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0601 - val_loss: 0.0187 - val_accuracy: 0.0354\n",
      "Epoch 276/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0581 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 277/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0621 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 278/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0591 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 279/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0581 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 280/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0591 - val_loss: 0.0192 - val_accuracy: 0.0354\n",
      "Epoch 281/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0631 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 282/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0631 - val_loss: 0.0188 - val_accuracy: 0.0354\n",
      "Epoch 283/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0640 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 284/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0640 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 285/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0621 - val_loss: 0.0186 - val_accuracy: 0.0354\n",
      "Epoch 286/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0631 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0631 - val_loss: 0.0190 - val_accuracy: 0.0354\n",
      "Epoch 288/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0591 - val_loss: 0.0200 - val_accuracy: 0.0354\n",
      "Epoch 289/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0591 - val_loss: 0.0187 - val_accuracy: 0.0354\n",
      "Epoch 290/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0631 - val_loss: 0.0194 - val_accuracy: 0.0354\n",
      "Epoch 291/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0621 - val_loss: 0.0193 - val_accuracy: 0.0354\n",
      "Epoch 292/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0631 - val_loss: 0.0184 - val_accuracy: 0.0354\n",
      "Epoch 293/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0631 - val_loss: 0.0189 - val_accuracy: 0.0354\n",
      "Epoch 294/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0631 - val_loss: 0.0186 - val_accuracy: 0.0354\n",
      "Epoch 295/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0631 - val_loss: 0.0185 - val_accuracy: 0.0354\n",
      "Epoch 296/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0640 - val_loss: 0.0195 - val_accuracy: 0.0354\n",
      "Epoch 297/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.0640 - val_loss: 0.0183 - val_accuracy: 0.0354\n",
      "Epoch 298/300\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0611 - val_loss: 0.0191 - val_accuracy: 0.0354\n",
      "Epoch 299/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0601 - val_loss: 0.0188 - val_accuracy: 0.0354\n",
      "Epoch 300/300\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0631 - val_loss: 0.0185 - val_accuracy: 0.0354\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(          0         1         2         3         4         5         6    \\\n",
       " 0    0.998915  0.922465  0.536514  0.539668  0.255824  0.251821  0.544856   \n",
       " 1    0.999242  0.919320  0.562400  0.492529  0.258144  0.255682  0.562533   \n",
       " 2    0.999065  0.908250  0.457159  0.602322  0.245868  0.241458  0.589049   \n",
       " 3    0.989973  0.374943  0.778346  0.733983  0.163829  0.159629  0.724788   \n",
       " 4   -0.051846  0.423783  0.819495  0.658549  0.221633  0.217478  0.590630   \n",
       " ..        ...       ...       ...       ...       ...       ...       ...   \n",
       " 751 -0.057241  0.959133  0.072455  0.134875  0.459080  0.456522  0.224515   \n",
       " 752  0.045654  0.693847  0.063322  0.506772  0.398667  0.393929  0.337410   \n",
       " 753 -0.016854  0.882963  0.575130  0.279986  0.413091  0.409780  0.273414   \n",
       " 754 -0.059465  0.885401  0.670559  0.397276  0.372936  0.370277  0.334912   \n",
       " 755  0.040375  0.939237  0.745015  0.363971  0.385752  0.382147  0.321641   \n",
       " \n",
       "           7         8         9    ...       743       744       745  \\\n",
       " 0    0.012561  0.074318  0.068012  ... -0.006641 -0.049860  0.002228   \n",
       " 1    0.006738  0.059930  0.060155  ...  0.007933  0.029100  0.048799   \n",
       " 2    0.012947  0.062823  0.065580  ... -0.007804 -0.025725 -0.003374   \n",
       " 3    0.309196  0.198564  0.191758  ...  0.021569 -0.016088 -0.003088   \n",
       " 4    0.324741  0.180350  0.172666  ...  0.061068  0.057418  0.024717   \n",
       " ..        ...       ...       ...  ...       ...       ...       ...   \n",
       " 751 -0.007459  0.023628  0.006545  ...  0.039395 -0.006101 -0.004371   \n",
       " 752  0.193095  0.048940  0.031936  ...  0.007058  0.006754  0.003474   \n",
       " 753  0.003913  0.017550  0.012053  ...  0.135369  0.225686  0.121569   \n",
       " 754  0.000111  0.033422  0.034703  ...  0.023939  0.052153 -0.012705   \n",
       " 755 -0.011621  0.025353  0.020776  ...  0.042359  0.222230  0.114201   \n",
       " \n",
       "           746       747       748       749       750       751       752  \n",
       " 0    0.054048  0.040385  0.045269  0.050841  0.075046  0.078979  0.227399  \n",
       " 1    0.128223  0.079085  0.052052  0.061593  0.155799  0.144116  0.268343  \n",
       " 2    0.012816  0.005213  0.014575  0.024304  0.101987  0.146815  0.238544  \n",
       " 3   -0.003689  0.044192  0.036803  0.059482  0.046410  0.064207  0.050069  \n",
       " 4    0.036445  0.043998  0.039407  0.056134  0.026516  0.031479  0.028488  \n",
       " ..        ...       ...       ...       ...       ...       ...       ...  \n",
       " 751  0.026909  0.052710  0.132354  0.082441  0.085714  0.118549  0.017643  \n",
       " 752  0.033221  0.028166  0.032329  0.036906  0.062793  0.087533  0.063975  \n",
       " 753  0.131728  0.062944  0.017708  0.017645  0.057442  0.039966  0.027358  \n",
       " 754  0.012178  0.031928  0.025159  0.017405 -0.001672 -0.041934 -0.026386  \n",
       " 755  0.107169  0.053221  0.028773  0.025779  0.037917 -0.005533  0.004071  \n",
       " \n",
       " [756 rows x 753 columns],\n",
       " 756)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def auto_encoder(x_train):\n",
    "    from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import SGD, Adam\n",
    "\n",
    "    # create the autoencoder.\n",
    "    model = Sequential()\n",
    "    # add input layer\n",
    "    model.add(Input((x_train.shape[1],)))\n",
    "\n",
    "    # add the encoding layer.\n",
    "    model.add(Dense(250, activation='relu')) # Number of PC (PCA) to include 99% variance of data \n",
    "#     model.add(Dense(100, activation='relu')) # Number of PC (PCA) to include 90% variance of PCA_DATA \n",
    "    \n",
    "    # bottle neck of auto encoder\n",
    "    model.add(Dense(80, activation='relu')) \n",
    "    \n",
    "    # add the decoding layer\n",
    "#     model.add(Dense(100, activation='relu')) \n",
    "    model.add(Dense(250, activation='relu'))\n",
    "    \n",
    "    # add the output layer.\n",
    "    model.add(Dense(x_train.shape[1], activation='tanh'))\n",
    "\n",
    "    number_of_epochs = 300; lr = 0.001\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mae', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, x_train, epochs=number_of_epochs, batch_size=16, validation_split=0.1)\n",
    "    \n",
    "    params = history.history\n",
    "    plt.plot(range(number_of_epochs), params['loss'])\n",
    "    plt.plot(range(number_of_epochs), params['val_loss'])\n",
    "    plt.show()\n",
    "    plt.plot(range(number_of_epochs), params['accuracy'])\n",
    "    plt.plot(range(number_of_epochs), params['val_accuracy'])\n",
    "    plt.show()\n",
    "    \n",
    "    out_data = model.predict(X)\n",
    "    return out_data\n",
    "    \n",
    "temp = auto_encoder(balance_data)    \n",
    "pd.DataFrame(temp), len(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Backward Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbf(in_data, target, n_component):\n",
    "    from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    number_of_features = n_component\n",
    "    sfs1=sfs(KNeighborsClassifier(n_neighbors=3),k_features=number_of_features, forward=False,\n",
    "             floating=False, verbose=3, scoring='roc_auc', cv=3)\n",
    "    sfs1=sfs1.fit(in_data, target)\n",
    "\n",
    "    print('\\nSequential Forward Floating Selection (k=3):')\n",
    "    print(sffs.k_feature_idx_)\n",
    "    print('CV Score:')\n",
    "    print(sffs.k_score_)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentional Reduction Techniqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimention_reduction(in_data, in_target, method, n_components = None, whitening = False, plot = False):\n",
    "    if method == 'PCA':\n",
    "        return pca_func(in_data, n_components, whitening = whitening, svd_solver = 'full', plot = plot)\n",
    "    elif method == 'LDA':\n",
    "        return lda_func(in_data, in_target, 1, solver = 'svd')\n",
    "    elif method == 'ICA':\n",
    "        return ica_func(in_data, n_components)\n",
    "    elif method == 'autoencoder':\n",
    "        return auto_encoder(in_data)\n",
    "    elif method == 'sbf':\n",
    "        return sbf(in_data, target, n_component)\n",
    "    else:\n",
    "        return in_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_train_test(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    print('Test Data ', Counter(y_test), 'Train Data ', Counter(y_train))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X_train, X_test, y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    clf = LogisticRegression(C=100, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n",
    "\n",
    "# _, y_pred = logistic(X_train, X_test, y_train)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(X_train, X_test, y_train):\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    clf = SVC(C=1, kernel='poly', degree=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n",
    "\n",
    "# _, y_pred = svc(X_train, X_test, y_train)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(X_train, X_test, y_train):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n",
    "\n",
    "# _, y_pred = tree(X_train, X_test, y_train)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n",
    "\n",
    "# _, y_pred = tree(X_train, X_test, y_train)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, X_test, y_train, y_test):\n",
    "    from keras.layers import Dense, Input, Dropout\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import SGD, Adam\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # create the autoencoder.\n",
    "    model = Sequential()\n",
    "    # add input layer\n",
    "    model.add(Input((X_train.shape[1],)))\n",
    "\n",
    "    # add the encoding layer.\n",
    "    model.add(Dense(50, activation='relu')) # Number of PC (PCA) to include 99% variance of data \n",
    "    model.add(Dense(10, activation='relu')) # Number of PC (PCA) to include 90% variance of PCA_DATA \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # add the output layer.\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    number_of_epochs = 20; lr = 0.001\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=number_of_epochs, batch_size=1, validation_data=(X_test, y_test))\n",
    "    \n",
    "    params = history.history\n",
    "    plt.plot(range(number_of_epochs), params['loss'])\n",
    "    plt.plot(range(number_of_epochs), params['val_loss'])\n",
    "    plt.title('loss')\n",
    "    plt.show()\n",
    "    plt.plot(range(number_of_epochs), params['accuracy'])\n",
    "    plt.plot(range(number_of_epochs), params['val_accuracy'])\n",
    "    plt.title('accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    return model, y_pred\n",
    "    \n",
    "# _, y_pred = mlp(X_train, X_test, y_train, y_test)\n",
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, units, gamma, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "#         print(input_shape)\n",
    "#         print(self.units)\n",
    "        self.mu = self.add_weight(name='mu',\n",
    "                                  shape=(int(input_shape[1]), self.units),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        diff = K.expand_dims(inputs) - self.mu\n",
    "        l2 = K.sum(K.pow(diff, 2), axis=1)\n",
    "        res = K.exp(-1 * self.gamma * l2)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rbf(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    from keras.layers import Dense, Flatten, Input\n",
    "    from keras.models import Sequential\n",
    "    from keras.losses import binary_crossentropy\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    \n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(RBFLayer(10, 0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, batch_size=1, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "    params = history.history\n",
    "    plt.plot(range(10), params['loss'])\n",
    "    plt.plot(range(10), params['val_loss'])\n",
    "    plt.title('loss')\n",
    "    plt.show()\n",
    "    plt.plot(range(10), params['accuracy'])\n",
    "    plt.plot(range(10), params['val_accuracy'])\n",
    "    plt.title('accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminative_method(X_train, X_test, y_train, y_test, method):\n",
    "    if method == 'logistic':\n",
    "        return logistic(X_train, X_test, y_train)\n",
    "    elif method == 'svm':\n",
    "        return svm(X_train, X_test, y_train)\n",
    "    elif method == 'tree':\n",
    "        return tree(X_train, X_test, y_train)\n",
    "    elif method == 'knn':\n",
    "        return knn(X_train, X_test, y_train)\n",
    "    elif method == 'mlp':\n",
    "        return mlp(X_train, X_test, y_train, y_test)\n",
    "    elif method == 'rbf':\n",
    "        return rbf(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM Model\n",
    "from sklearn.mixture import GaussianMixture\n",
    "def gmm_classifier(X_train, X_test, y_train, components = 2, covariance_type = 'full'):\n",
    "    clf = GaussianMixture(n_components=components, covariance_type = covariance_type)\n",
    "    clf.fit(X_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parzen Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    A Naive Bayes Classifier using Kernel Density Estimation with Parzen windows.\n",
    "    The classifier implements two kernels for parzen window  - Radial and Hypercube\n",
    "    It also implements Single bandwidth model and class-specific Multi bandwidth model\n",
    "    The kernel and model type are passed as arguments to class object initialization.\n",
    "    Along with the number of bandwidths necessary, in case of Multi bandwidth model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bandwidth=1, kernel='radial', multi_bw=False):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with proper parameters.\n",
    "        :param bandwidth: An integer giving the number of bandwidths necessary\n",
    "        :param kernel: A string specifying the kernel to be used for the model\n",
    "        :param multi_bw: A boolean variable specifying if the Multi bandwidth\n",
    "                        model is to be used.\n",
    "                        By default Single bandwidth model is selected.\n",
    "        \"\"\"\n",
    "        self.priors = dict()\n",
    "        self.dim = 1\n",
    "        self.multi_bw = multi_bw\n",
    "        self.bandwidth = bandwidth\n",
    "        if kernel == \"radial\":\n",
    "            self.kernel = self.radial\n",
    "        elif kernel == \"hypercube\":\n",
    "            self.kernel = self.hypercube\n",
    "\n",
    "    def hypercube(self, k):\n",
    "        \"\"\"\n",
    "        Hypercube kernel for Density Estimation.\n",
    "        \"\"\"\n",
    "        return np.all(k < 0.5, axis=1)\n",
    "\n",
    "    def radial(self, k):\n",
    "        \"\"\"\n",
    "        Radial Kernel for Density estimation.\n",
    "        \"\"\"\n",
    "        const_part = (2 * np.pi) ** (-self.dim / 2)\n",
    "        return const_part * np.exp(-0.5 * np.add.reduce(k ** 2, axis=1))\n",
    "\n",
    "    def parzen_estimation(self, h, x, x_train):\n",
    "        \"\"\"\n",
    "        Density estimation for a single sample against the training set with\n",
    "        parzen window using the specified bandwidth, kernel.\n",
    "        :param h: An integer value giving the bandwidth to be used for the class.\n",
    "        :param x: A single input sample, whose density needs to be estimated.\n",
    "        :param x_train: Array of input data to calculate KDE value against.\n",
    "        :return: A single float value giving the density of the function at the given point.\n",
    "        \"\"\"\n",
    "        N = x_train.shape[0]\n",
    "        dim = self.dim\n",
    "        k = np.abs(x - x_train) * 1.0 / h\n",
    "        summation = np.add.reduce(self.kernel(k))\n",
    "        return summation / (N * (h ** dim))\n",
    "\n",
    "    def KDE(self, h, x_test, x_train):\n",
    "        \"\"\"\n",
    "        Kernel Density Estimation based on the parameters set.\n",
    "        :param h: An integer value giving the bandwidth to be used for the class.\n",
    "        :param x_test: Array of input data to make predictions.\n",
    "        :param x_train: Array of input data to calculate KDE value against.\n",
    "        :return: A list of floats giving the density estimation values for each\n",
    "                 row in x_test, x_test[i] calculated against the training set, previously set\n",
    "        \"\"\"\n",
    "        P_x = np.zeros(len(x_test))\n",
    "        N = x_train.shape[0]\n",
    "        dim = self.dim\n",
    "        for i in range(len(x_test)):\n",
    "            P_x[i] = self.parzen_estimation(h, x_test[i], x_train)\n",
    "\n",
    "        return P_x\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fits the model to the training set.\n",
    "        Since KDE is a lazy learner we just need to save the necessary information.\n",
    "        :param X: Array of input data\n",
    "        :param Y: Array of output labels\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.x_train = X\n",
    "        self.y_train = Y\n",
    "        self.dim = X.shape[1]\n",
    "        labels = set(Y)\n",
    "        for c in labels:\n",
    "            self.priors[c] = float(len(Y[Y == c])) / len(Y)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of testing set, using KDE.\n",
    "        :param x_test: Array of input data to make predictions.\n",
    "        :return: Predicted labels of the data.\n",
    "        \"\"\"\n",
    "        N, D = x_test.shape\n",
    "        priors = self.priors\n",
    "        K = len(priors)\n",
    "        P = np.zeros((N, K))\n",
    "        x_train = self.x_train\n",
    "        y_train = self.y_train\n",
    "        if self.multi_bw:\n",
    "            bw = self.bandwidth\n",
    "        else:\n",
    "            bw = np.repeat(self.bandwidth, K)\n",
    "        for c, p in priors.items():\n",
    "            P[:, int(c)] = self.KDE(bw[int(c)], x_test, x_train[y_train == c]) * p\n",
    "\n",
    "        pred_y = np.argmax(P, axis=1)\n",
    "        self.pred_y = pred_y\n",
    "\n",
    "        return pred_y\n",
    "\n",
    "    def accuracy(self, y_test):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy between the predicted label and actual labels.\n",
    "        :param y_test: Array of actual output labels of Testing set.\n",
    "        :return: A float value giving the accuracy.\n",
    "        \"\"\"\n",
    "        pred_y = self.pred_y\n",
    "        return np.array([pred_y == y_test]).mean()\n",
    "\n",
    "    def score(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Function that runs both Predict and Accuracy and returns the accuracy\n",
    "        score of the model.\n",
    "        :param x_test: Array of input data to make predictions.\n",
    "        :param y_test: Array of actual output labels of Testing set.\n",
    "        :return: A float value giving the accuracy of the model.\n",
    "        \"\"\"\n",
    "        self.predict(x_test)\n",
    "        return self.accuracy(y_test)\n",
    "    \n",
    "def BC_Parzen(x_train, y_train, x_test):\n",
    "    print(x_train)\n",
    "    nbmodel = NaiveBayesClassifier(bandwidth=1,kernel='hypercube')\n",
    "    nbmodel.fit(np.array(x_train), np.array(y_train))\n",
    "    pre = nbmodel.predict(np.array(x_test))\n",
    "    pre = pd.DataFrame(pre)\n",
    "    return nbmodel, pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier_KNN_Base:\n",
    "    \"\"\"\n",
    "    A Naive Bayes Classifier using Kernel Density Estimation with Parzen windows.\n",
    "    The classifier implements two kernels for parzen window  - Radial and Hypercube\n",
    "    It also implements Single bandwidth model and class-specific Multi bandwidth model\n",
    "    The kernel and model type are passed as arguments to class object initialization.\n",
    "    Along with the number of bandwidths necessary, in case of Multi bandwidth model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bandwidth=1, kernel='radial', multi_bw=False):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with proper parameters.\n",
    "        :param bandwidth: An integer giving the number of bandwidths necessary\n",
    "        :param kernel: A string specifying the kernel to be used for the model\n",
    "        :param multi_bw: A boolean variable specifying if the Multi bandwidth\n",
    "                        model is to be used.\n",
    "                        By default Single bandwidth model is selected.\n",
    "        \"\"\"\n",
    "        self.priors = dict()\n",
    "        self.dim = 1\n",
    "        self.multi_bw = multi_bw\n",
    "        self.bandwidth = bandwidth\n",
    "        if kernel == \"radial\":\n",
    "            self.kernel = self.radial\n",
    "        elif kernel == \"hypercube\":\n",
    "            self.kernel = self.hypercube\n",
    "\n",
    "    def hypercube(self, k):\n",
    "        \"\"\"\n",
    "        Hypercube kernel for Density Estimation.\n",
    "        \"\"\"\n",
    "        return np.all(k < 0.5, axis=1)\n",
    "\n",
    "    def radial(self, k):\n",
    "        \"\"\"\n",
    "        Radial Kernel for Density estimation.\n",
    "        \"\"\"\n",
    "        const_part = (2 * np.pi) ** (-self.dim / 2)\n",
    "        return const_part * np.exp(-0.5 * np.add.reduce(k ** 2, axis=1))\n",
    "\n",
    "    def area(data):\n",
    "        size = 40\n",
    "        X = []\n",
    "        for i in range(data.shape[1]):\n",
    "            x=np.linspace(-4, 4, size)\n",
    "            X.append(x)\n",
    "        return np.array(X)    \n",
    "\n",
    "\n",
    "    def knn(data, k):\n",
    "        X = area(data)\n",
    "        size = [len(X[0]), len(X[1])]\n",
    "        knnpdf = np.zeros(size)\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                x = np.array([X[0][i],X[1][j]])\n",
    "                ds = [np.linalg.norm(x-y) for y in data]\n",
    "                ds.sort()\n",
    "                v = math.pi*ds[k-1]*ds[k-1]\n",
    "                if v == 0:\n",
    "                    knnpdf[i,j] = 1\n",
    "                else:\n",
    "                    knnpdf[i,j] = k/(n*v)\n",
    "        return knnpdf\n",
    "\n",
    "    # def parzen_estimation(self, h, x, x_train):\n",
    "    #     \"\"\"\n",
    "    #     Density estimation for a single sample against the training set with\n",
    "    #     parzen window using the specified bandwidth, kernel.\n",
    "    #     :param h: An integer value giving the bandwidth to be used for the class.\n",
    "    #     :param x: A single input sample, whose density needs to be estimated.\n",
    "    #     :param x_train: Array of input data to calculate KDE value against.\n",
    "    #     :return: A single float value giving the density of the function at the given point.\n",
    "    #     \"\"\"\n",
    "    #     N = x_train.shape[0]\n",
    "    #     dim = self.dim\n",
    "    #     k = np.abs(x - x_train) * 1.0 / h\n",
    "    #     summation = np.add.reduce(self.kernel(k))\n",
    "    #     return summation / (N * (h ** dim))\n",
    "\n",
    "    # def KDE(self, h, x_test, x_train):\n",
    "    #     \"\"\"\n",
    "    #     Kernel Density Estimation based on the parameters set.\n",
    "    #     :param h: An integer value giving the bandwidth to be used for the class.\n",
    "    #     :param x_test: Array of input data to make predictions.\n",
    "    #     :param x_train: Array of input data to calculate KDE value against.\n",
    "    #     :return: A list of floats giving the density estimation values for each\n",
    "    #              row in x_test, x_test[i] calculated against the training set, previously set\n",
    "    #     \"\"\"\n",
    "    #     P_x = np.zeros(len(x_test))\n",
    "    #     N = x_train.shape[0]\n",
    "    #     dim = self.dim\n",
    "    #     for i in range(len(x_test)):\n",
    "    #         P_x[i] = self.parzen_estimation(h, x_test[i], x_train)\n",
    "\n",
    "    #     return P_x\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fits the model to the training set.\n",
    "        Since KDE is a lazy learner we just need to save the necessary information.\n",
    "        :param X: Array of input data\n",
    "        :param Y: Array of output labels\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.x_train = X\n",
    "        self.y_train = Y\n",
    "        self.dim = X.shape[1]\n",
    "        labels = set(Y)\n",
    "        for c in labels:\n",
    "            self.priors[c] = float(len(Y[Y == c])) / len(Y)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of testing set, using KDE.\n",
    "        :param x_test: Array of input data to make predictions.\n",
    "        :return: Predicted labels of the data.\n",
    "        \"\"\"\n",
    "        N, D = x_test.shape\n",
    "        priors = self.priors\n",
    "        K = len(priors)\n",
    "        P = np.zeros((N, K))\n",
    "        x_train = self.x_train\n",
    "        y_train = self.y_train\n",
    "        if self.multi_bw:\n",
    "            bw = self.bandwidth\n",
    "        else:\n",
    "            bw = np.repeat(self.bandwidth, K)\n",
    "        for c, p in priors.items():\n",
    "            P[:, int(c)] = self.knn(k, x_test, x_train[y_train == c]) * p\n",
    "\n",
    "        pred_y = np.argmax(P, axis=1)\n",
    "        self.pred_y = pred_y\n",
    "\n",
    "        return pred_y\n",
    "\n",
    "    def accuracy(self, y_test):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy between the predicted label and actual labels.\n",
    "        :param y_test: Array of actual output labels of Testing set.\n",
    "        :return: A float value giving the accuracy.\n",
    "        \"\"\"\n",
    "        pred_y = self.pred_y\n",
    "        return np.array([pred_y == y_test]).mean()\n",
    "\n",
    "    def score(self, x_test, y_test):\n",
    "        \"\"\"\n",
    "        Function that runs both Predict and Accuracy and returns the accuracy\n",
    "        score of the model.\n",
    "        :param x_test: Array of input data to make predictions.\n",
    "        :param y_test: Array of actual output labels of Testing set.\n",
    "        :return: A float value giving the accuracy of the model.\n",
    "        \"\"\"\n",
    "        self.predict(x_test)\n",
    "        return self.accuracy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamlbe Classifers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "def bagging(X_train, X_test, y_train, estimator=SVC(), n_estimators=10):\n",
    "    clf = BaggingClassifier(base_estimator=estimator, n_estimators=n_estimators)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def forest(X_train, X_test, y_train, n_estimators=100, max_depth=5):\n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth=max_depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "def boost(X_train, X_test, y_train, n_estimators=100):\n",
    "    clf = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "def voting(X_train, X_test, y_train, estimators, voting='hard'):\n",
    "    clf = VotingClassifier(estimators=estimators, voting=voting)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    return clf, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def ensemble_model(X_train, X_test, y_train, method, estimator=SVC(), n_estimators=100, estimators=[], max_depth=5):\n",
    "    if method == 'bagging':\n",
    "        return bagging(X_train, X_test, y_train, estimator=estimator, n_estimators=10)\n",
    "    elif method == 'forest':\n",
    "        return forest(X_train, X_test, y_train, n_estimators=n_estimators, max_depth=max_depth)\n",
    "    elif method == 'boost':\n",
    "        return boost(X_train, X_test, y_train, n_estimators=n_estimators)\n",
    "    elif method == 'voting':\n",
    "        return voting(X_train, X_test, y_train, estimators, voting='hard')\n",
    "    else:\n",
    "        raise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, plot_roc_curve, roc_curve, auc, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def classification_reports(x_train, X_test, y_train, y_pred, y_test, model, target_names=['0', '1'], cv=5):\n",
    "    # Train Accuracy\n",
    "    print('Train accuracy on Train data is: ', accuracy_score(y_train, model.predict(x_train)))\n",
    "    \n",
    "    # Accuracy, precision, Recall, and F1-Score\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Cross Validation Score\n",
    "    scores = cross_val_score(model, x_train, y_train, cv=cv)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f with Cross Validation\" % (scores.mean(), scores.std()))\n",
    "    \n",
    "    # Roc Curve\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.show()\n",
    "    \n",
    "    # Area Under ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    print('Area under the ROC curve is : ', auc(fpr, tpr))\n",
    "    \n",
    "    plot_confusion_matrix(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>0</td>\n",
       "      <td>0.80903</td>\n",
       "      <td>0.56355</td>\n",
       "      <td>0.28385</td>\n",
       "      <td>417</td>\n",
       "      <td>416</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0706</td>\n",
       "      <td>3.0190</td>\n",
       "      <td>3.1212</td>\n",
       "      <td>2.4921</td>\n",
       "      <td>3.5844</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>3.3805</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>6.8671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0</td>\n",
       "      <td>0.16084</td>\n",
       "      <td>0.56499</td>\n",
       "      <td>0.59194</td>\n",
       "      <td>415</td>\n",
       "      <td>413</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9704</td>\n",
       "      <td>1.7451</td>\n",
       "      <td>1.8277</td>\n",
       "      <td>2.4976</td>\n",
       "      <td>5.2981</td>\n",
       "      <td>4.2616</td>\n",
       "      <td>6.3042</td>\n",
       "      <td>10.9058</td>\n",
       "      <td>28.4170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>0.88389</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.46815</td>\n",
       "      <td>381</td>\n",
       "      <td>380</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>51.5607</td>\n",
       "      <td>44.4641</td>\n",
       "      <td>26.1586</td>\n",
       "      <td>6.3076</td>\n",
       "      <td>2.8601</td>\n",
       "      <td>2.5361</td>\n",
       "      <td>3.5377</td>\n",
       "      <td>3.3545</td>\n",
       "      <td>5.0424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83782</td>\n",
       "      <td>0.74890</td>\n",
       "      <td>0.49823</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>19.1607</td>\n",
       "      <td>12.8312</td>\n",
       "      <td>8.9434</td>\n",
       "      <td>2.2044</td>\n",
       "      <td>1.9496</td>\n",
       "      <td>1.9664</td>\n",
       "      <td>2.6801</td>\n",
       "      <td>2.8332</td>\n",
       "      <td>3.7131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0</td>\n",
       "      <td>0.81304</td>\n",
       "      <td>0.76471</td>\n",
       "      <td>0.46374</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>62.9927</td>\n",
       "      <td>21.8152</td>\n",
       "      <td>9.2457</td>\n",
       "      <td>4.8555</td>\n",
       "      <td>3.0551</td>\n",
       "      <td>3.0415</td>\n",
       "      <td>4.0116</td>\n",
       "      <td>2.6217</td>\n",
       "      <td>3.1527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows  754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0         1  0.85247  0.71826  0.57227        240               239   \n",
       "1         1  0.76686  0.69481  0.53966        234               233   \n",
       "2         1  0.85083  0.67604  0.58982        232               231   \n",
       "3         0  0.41121  0.79672  0.59257        178               177   \n",
       "4         0  0.32790  0.79782  0.53028        236               235   \n",
       "..      ...      ...      ...      ...        ...               ...   \n",
       "751       0  0.80903  0.56355  0.28385        417               416   \n",
       "752       0  0.16084  0.56499  0.59194        415               413   \n",
       "753       0  0.88389  0.72335  0.46815        381               380   \n",
       "754       0  0.83782  0.74890  0.49823        340               339   \n",
       "755       0  0.81304  0.76471  0.46374        340               339   \n",
       "\n",
       "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n",
       "0            0.008064            0.000087       0.00218      0.000018  ...   \n",
       "1            0.008258            0.000073       0.00195      0.000016  ...   \n",
       "2            0.008340            0.000060       0.00176      0.000015  ...   \n",
       "3            0.010858            0.000183       0.00419      0.000046  ...   \n",
       "4            0.008162            0.002669       0.00535      0.000044  ...   \n",
       "..                ...                 ...           ...           ...  ...   \n",
       "751          0.004627            0.000052       0.00064      0.000003  ...   \n",
       "752          0.004550            0.000220       0.00143      0.000006  ...   \n",
       "753          0.005069            0.000103       0.00076      0.000004  ...   \n",
       "754          0.005679            0.000055       0.00092      0.000005  ...   \n",
       "755          0.005676            0.000037       0.00078      0.000004  ...   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                       1.5620                     2.6445   \n",
       "1                       1.5589                     3.6107   \n",
       "2                       1.5643                     2.3308   \n",
       "3                       3.7805                     3.5664   \n",
       "4                       6.1727                     5.8416   \n",
       "..                         ...                        ...   \n",
       "751                     3.0706                     3.0190   \n",
       "752                     1.9704                     1.7451   \n",
       "753                    51.5607                    44.4641   \n",
       "754                    19.1607                    12.8312   \n",
       "755                    62.9927                    21.8152   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                       3.8686                     4.2105   \n",
       "1                      23.5155                    14.1962   \n",
       "2                       9.4959                    10.7458   \n",
       "3                       5.2558                    14.0403   \n",
       "4                       6.0805                     5.7621   \n",
       "..                         ...                        ...   \n",
       "751                     3.1212                     2.4921   \n",
       "752                     1.8277                     2.4976   \n",
       "753                    26.1586                     6.3076   \n",
       "754                     8.9434                     2.2044   \n",
       "755                     9.2457                     4.8555   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                       5.1221                     4.4625   \n",
       "1                      11.0261                     9.5082   \n",
       "2                      11.0177                     4.8066   \n",
       "3                       4.2235                     4.6857   \n",
       "4                       7.7817                    11.6891   \n",
       "..                         ...                        ...   \n",
       "751                     3.5844                     3.5400   \n",
       "752                     5.2981                     4.2616   \n",
       "753                     2.8601                     2.5361   \n",
       "754                     1.9496                     1.9664   \n",
       "755                     3.0551                     3.0415   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                       2.6202                     3.0004   \n",
       "1                       6.5245                     6.3431   \n",
       "2                       2.9199                     3.1495   \n",
       "3                       4.8460                     6.2650   \n",
       "4                       8.2103                     5.0559   \n",
       "..                         ...                        ...   \n",
       "751                     3.3805                     3.2003   \n",
       "752                     6.3042                    10.9058   \n",
       "753                     3.5377                     3.3545   \n",
       "754                     2.6801                     2.8332   \n",
       "755                     4.0116                     2.6217   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_36  class  \n",
       "0                      18.9405      1  \n",
       "1                      45.1780      1  \n",
       "2                       4.7666      1  \n",
       "3                       4.0603      1  \n",
       "4                       6.1164      1  \n",
       "..                         ...    ...  \n",
       "751                     6.8671      0  \n",
       "752                    28.4170      0  \n",
       "753                     5.0424      0  \n",
       "754                     3.7131      0  \n",
       "755                     3.1527      0  \n",
       "\n",
       "[756 rows x 754 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 756 entries, 0 to 755\n",
      "Columns: 754 entries, gender to class\n",
      "dtypes: float64(749), int64(5)\n",
      "memory usage: 4.3 MB\n",
      "None Counter({1: 564, 0: 192})\n",
      "\n",
      "Normilize Data: \n",
      "\n",
      "Dimentional Reduction: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.339397</td>\n",
       "      <td>-0.558285</td>\n",
       "      <td>-1.002495</td>\n",
       "      <td>-0.157676</td>\n",
       "      <td>-0.213265</td>\n",
       "      <td>-0.243279</td>\n",
       "      <td>0.212460</td>\n",
       "      <td>-0.493192</td>\n",
       "      <td>-0.061899</td>\n",
       "      <td>0.053725</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026636</td>\n",
       "      <td>-0.008193</td>\n",
       "      <td>-0.117116</td>\n",
       "      <td>-0.041140</td>\n",
       "      <td>-0.044986</td>\n",
       "      <td>-0.016053</td>\n",
       "      <td>-0.058132</td>\n",
       "      <td>-0.017714</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.049666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.450711</td>\n",
       "      <td>-0.493887</td>\n",
       "      <td>-1.106028</td>\n",
       "      <td>0.170330</td>\n",
       "      <td>-0.217101</td>\n",
       "      <td>-0.205296</td>\n",
       "      <td>0.228974</td>\n",
       "      <td>-0.385226</td>\n",
       "      <td>0.329423</td>\n",
       "      <td>-0.227700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079149</td>\n",
       "      <td>0.025625</td>\n",
       "      <td>0.070407</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.071202</td>\n",
       "      <td>-0.011807</td>\n",
       "      <td>-0.017480</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>-0.156249</td>\n",
       "      <td>-0.021781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.923121</td>\n",
       "      <td>-0.177777</td>\n",
       "      <td>-0.940145</td>\n",
       "      <td>0.251750</td>\n",
       "      <td>-0.136722</td>\n",
       "      <td>-0.543182</td>\n",
       "      <td>0.291437</td>\n",
       "      <td>-0.460222</td>\n",
       "      <td>-0.158691</td>\n",
       "      <td>0.114233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012915</td>\n",
       "      <td>-0.005679</td>\n",
       "      <td>-0.114578</td>\n",
       "      <td>-0.059765</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>-0.034148</td>\n",
       "      <td>-0.051747</td>\n",
       "      <td>0.113740</td>\n",
       "      <td>-0.045294</td>\n",
       "      <td>0.015596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.379726</td>\n",
       "      <td>-1.763046</td>\n",
       "      <td>1.825072</td>\n",
       "      <td>-0.445533</td>\n",
       "      <td>-1.132919</td>\n",
       "      <td>0.672383</td>\n",
       "      <td>-1.663618</td>\n",
       "      <td>-0.479242</td>\n",
       "      <td>0.216811</td>\n",
       "      <td>-0.263052</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075131</td>\n",
       "      <td>-0.132755</td>\n",
       "      <td>-0.097237</td>\n",
       "      <td>0.043989</td>\n",
       "      <td>-0.137158</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>-0.106942</td>\n",
       "      <td>-0.009666</td>\n",
       "      <td>0.094749</td>\n",
       "      <td>-0.095522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.155739</td>\n",
       "      <td>-1.186710</td>\n",
       "      <td>1.922939</td>\n",
       "      <td>-0.795982</td>\n",
       "      <td>-1.262255</td>\n",
       "      <td>0.876244</td>\n",
       "      <td>-1.479921</td>\n",
       "      <td>-0.593400</td>\n",
       "      <td>-0.001184</td>\n",
       "      <td>-0.091413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005393</td>\n",
       "      <td>0.169948</td>\n",
       "      <td>0.112602</td>\n",
       "      <td>0.050603</td>\n",
       "      <td>0.064056</td>\n",
       "      <td>0.050452</td>\n",
       "      <td>-0.056014</td>\n",
       "      <td>-0.061351</td>\n",
       "      <td>0.050649</td>\n",
       "      <td>-0.021486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>3.325712</td>\n",
       "      <td>-0.102974</td>\n",
       "      <td>-0.077957</td>\n",
       "      <td>1.445495</td>\n",
       "      <td>-0.808284</td>\n",
       "      <td>-1.123834</td>\n",
       "      <td>-0.501540</td>\n",
       "      <td>0.611596</td>\n",
       "      <td>0.619795</td>\n",
       "      <td>-0.239251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218928</td>\n",
       "      <td>0.128203</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>-0.163980</td>\n",
       "      <td>0.129897</td>\n",
       "      <td>0.112412</td>\n",
       "      <td>-0.188272</td>\n",
       "      <td>-0.054158</td>\n",
       "      <td>0.131110</td>\n",
       "      <td>-0.078327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>2.169707</td>\n",
       "      <td>0.244748</td>\n",
       "      <td>1.182231</td>\n",
       "      <td>1.732952</td>\n",
       "      <td>-0.680034</td>\n",
       "      <td>-0.403690</td>\n",
       "      <td>-0.615912</td>\n",
       "      <td>0.184044</td>\n",
       "      <td>-0.282782</td>\n",
       "      <td>-0.550383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250667</td>\n",
       "      <td>0.138780</td>\n",
       "      <td>0.113890</td>\n",
       "      <td>-0.016317</td>\n",
       "      <td>0.199461</td>\n",
       "      <td>0.049861</td>\n",
       "      <td>0.225782</td>\n",
       "      <td>-0.016176</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>-0.054839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>1.203056</td>\n",
       "      <td>-0.043257</td>\n",
       "      <td>-0.011547</td>\n",
       "      <td>-0.673853</td>\n",
       "      <td>-0.147413</td>\n",
       "      <td>-0.369289</td>\n",
       "      <td>-0.020674</td>\n",
       "      <td>-0.045992</td>\n",
       "      <td>0.535601</td>\n",
       "      <td>-0.084465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017337</td>\n",
       "      <td>0.115875</td>\n",
       "      <td>0.033992</td>\n",
       "      <td>-0.040753</td>\n",
       "      <td>-0.003842</td>\n",
       "      <td>0.051217</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>-0.107646</td>\n",
       "      <td>0.057116</td>\n",
       "      <td>-0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>0.787112</td>\n",
       "      <td>-0.627686</td>\n",
       "      <td>0.101777</td>\n",
       "      <td>-0.850750</td>\n",
       "      <td>0.022926</td>\n",
       "      <td>-0.505056</td>\n",
       "      <td>-0.439549</td>\n",
       "      <td>-0.205641</td>\n",
       "      <td>0.244467</td>\n",
       "      <td>-0.331640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037253</td>\n",
       "      <td>0.046011</td>\n",
       "      <td>0.097694</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>0.085136</td>\n",
       "      <td>0.048345</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.143496</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>-0.057061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0.723926</td>\n",
       "      <td>-0.707669</td>\n",
       "      <td>-0.031763</td>\n",
       "      <td>-0.940631</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>-0.378396</td>\n",
       "      <td>-0.678956</td>\n",
       "      <td>0.041359</td>\n",
       "      <td>0.575689</td>\n",
       "      <td>-0.225400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051608</td>\n",
       "      <td>-0.067308</td>\n",
       "      <td>-0.036628</td>\n",
       "      <td>0.058103</td>\n",
       "      <td>0.086641</td>\n",
       "      <td>0.027187</td>\n",
       "      <td>0.077042</td>\n",
       "      <td>0.014072</td>\n",
       "      <td>-0.005471</td>\n",
       "      <td>-0.019475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows  131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -1.339397 -0.558285 -1.002495 -0.157676 -0.213265 -0.243279  0.212460   \n",
       "1   -1.450711 -0.493887 -1.106028  0.170330 -0.217101 -0.205296  0.228974   \n",
       "2   -1.923121 -0.177777 -0.940145  0.251750 -0.136722 -0.543182  0.291437   \n",
       "3   -0.379726 -1.763046  1.825072 -0.445533 -1.132919  0.672383 -1.663618   \n",
       "4   -0.155739 -1.186710  1.922939 -0.795982 -1.262255  0.876244 -1.479921   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "751  3.325712 -0.102974 -0.077957  1.445495 -0.808284 -1.123834 -0.501540   \n",
       "752  2.169707  0.244748  1.182231  1.732952 -0.680034 -0.403690 -0.615912   \n",
       "753  1.203056 -0.043257 -0.011547 -0.673853 -0.147413 -0.369289 -0.020674   \n",
       "754  0.787112 -0.627686  0.101777 -0.850750  0.022926 -0.505056 -0.439549   \n",
       "755  0.723926 -0.707669 -0.031763 -0.940631  0.573576 -0.378396 -0.678956   \n",
       "\n",
       "          7         8         9    ...       121       122       123  \\\n",
       "0   -0.493192 -0.061899  0.053725  ... -0.026636 -0.008193 -0.117116   \n",
       "1   -0.385226  0.329423 -0.227700  ...  0.079149  0.025625  0.070407   \n",
       "2   -0.460222 -0.158691  0.114233  ...  0.012915 -0.005679 -0.114578   \n",
       "3   -0.479242  0.216811 -0.263052  ... -0.075131 -0.132755 -0.097237   \n",
       "4   -0.593400 -0.001184 -0.091413  ...  0.005393  0.169948  0.112602   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "751  0.611596  0.619795 -0.239251  ... -0.218928  0.128203  0.094809   \n",
       "752  0.184044 -0.282782 -0.550383  ...  0.250667  0.138780  0.113890   \n",
       "753 -0.045992  0.535601 -0.084465  ... -0.017337  0.115875  0.033992   \n",
       "754 -0.205641  0.244467 -0.331640  ...  0.037253  0.046011  0.097694   \n",
       "755  0.041359  0.575689 -0.225400  ...  0.051608 -0.067308 -0.036628   \n",
       "\n",
       "          124       125       126       127       128       129       130  \n",
       "0   -0.041140 -0.044986 -0.016053 -0.058132 -0.017714  0.000768  0.049666  \n",
       "1    0.001571  0.071202 -0.011807 -0.017480  0.005554 -0.156249 -0.021781  \n",
       "2   -0.059765  0.084565 -0.034148 -0.051747  0.113740 -0.045294  0.015596  \n",
       "3    0.043989 -0.137158  0.002735 -0.106942 -0.009666  0.094749 -0.095522  \n",
       "4    0.050603  0.064056  0.050452 -0.056014 -0.061351  0.050649 -0.021486  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "751 -0.163980  0.129897  0.112412 -0.188272 -0.054158  0.131110 -0.078327  \n",
       "752 -0.016317  0.199461  0.049861  0.225782 -0.016176  0.150459 -0.054839  \n",
       "753 -0.040753 -0.003842  0.051217  0.046853 -0.107646  0.057116 -0.039392  \n",
       "754  0.074441  0.085136  0.048345  0.005308  0.143496  0.005923 -0.057061  \n",
       "755  0.058103  0.086641  0.027187  0.077042  0.014072 -0.005471 -0.019475  \n",
       "\n",
       "[756 rows x 131 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Test Split\n",
      "Test Data  Counter({1: 169, 0: 58}) Train Data  Counter({1: 395, 0: 134})\n",
      "\n",
      "Handle Imbalance Data: \n",
      "After Handling Imbalaced Data:  Counter({1: 395, 0: 395})\n",
      "Train accuracy on Train data is:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.48      0.47        58\n",
      "           1       0.82      0.81      0.82       169\n",
      "\n",
      "    accuracy                           0.73       227\n",
      "   macro avg       0.64      0.65      0.65       227\n",
      "weighted avg       0.73      0.73      0.73       227\n",
      "\n",
      "0.82 accuracy with a standard deviation of 0.02 with Cross Validation\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve is :  0.6467047541318098\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    print('\\nLoad Data: ')\n",
    "    X, y = load_data()\n",
    "    print('\\nNormilize Data: ')\n",
    "    X = normilize_data(X)\n",
    "\n",
    "    print('\\nDimentional Reduction: ')\n",
    "    X_low_dim = dimention_reduction(X, y, 'PCA', n_components = 131, whitening =False, plot = False) # PCA, LDA, ICA, autoencoder, sbf\n",
    "    display(pd.DataFrame(X_low_dim))\n",
    "\n",
    "    print('\\nTrain Test Split')\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X_low_dim, y)\n",
    "\n",
    "\n",
    "    print('\\nHandle Imbalance Data: ')\n",
    "    X_train, y_train = handle_imbalanced(X_train, y_train, 'smote', sampling_strategy= 'minority') # smote, oversample, undersample\n",
    "    print('After Handling Imbalaced Data: ',Counter(y_train))\n",
    "    \n",
    "    model , y_pred = discriminative_method(X_train, X_test, y_train, y_test, 'tree') # logistic, svm, tree, knn, mlp, rbf\n",
    "#     model , y_pred = ensemble_model(X_train, X_test, y_train, 'boost', estimator=DecisionTreeClassifier(), n_estimators=100, estimators=eclf1, max_depth=5)\n",
    "#     model , y_pred = gmm_classifier(X_train, X_test, y_train)\n",
    "    \n",
    "    classification_reports(X_train, X_test, y_train, y_pred, y_test, model, target_names=['0', '1'], cv=5)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dimention_method, n_components, whitening, classifier_method):\n",
    "    X,y = load_data()\n",
    "    X = normlize_data(X)\n",
    "\n",
    "    X_low_dim = dimention_reduction(X, y, dimention_method, n_components = n_components, whitening = whitening, plot = True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X_low_dim, y)\n",
    "\n",
    "    model , y_pred = discriminative_method(X_train, X_test, y_train, y_test, classifier_method)\n",
    "\n",
    "    classification_reports(x_train, y_train, y_pred, y_test, model, target_names=['0', '1'], cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
